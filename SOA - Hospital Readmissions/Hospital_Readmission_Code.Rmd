---
title: "Hospital Readmission template"
---


## Read Data

This chunk reads in the data, relevels factors, and prints a summary.

```{r}
# Loading data
readmission <- read.csv(file="readmission.csv")

vars <- colnames(readmission)[c(2,3,5,9)] #variables to relevel
for (i in vars){
  table <- as.data.frame(table(readmission[,i]))
  max <- which.max(table[,2])
  level.name <- as.character(table[max,1])
  readmission[,i] <- relevel(readmission[,i], ref = level.name)
}
summary(readmission)
str(readmission)
```

## Task 1

I have created histograms for all the numeric variables below

```{r}
library(ggplot2)

for(i in c(6:8)){
  m <- ggplot(data = readmission, aes(x = readmission[,i])) + geom_histogram() + labs(x =                                                                                         colnames(readmission)[i])
  print(m)
}

mean.median.table <- data.frame("variable" = c("LOS","Age","HCC.Riskscore"),
                                "mean" = c(mean(readmission$LOS), mean(readmission$Age), 
                                           mean(readmission$HCC.Riskscore)),
                                "median" = c(median(readmission$LOS), median(readmission$Age), 
                                           median(readmission$HCC.Riskscore)))

mean.median.table
write.csv(mean.median.table, "mean.median.table.csv")

```

We will create some new features / transformation

```{r}

# Create a backup
readmission1 <- readmission

# Take the Log of LOS and HCC.Riskscore
readmission1$Log_LOS <- log(readmission1$LOS)
readmission1$Log_HCC.Riskscore <- log(readmission1$HCC.Riskscore)

# remove the LOS and HCC.Riskscore variables
readmission1$LOS <- NULL
readmission1$HCC.Riskscore <- NULL

# Create a new feature for evaluating if age < 65
readmission1$Under_65 <- ifelse(readmission1$Age < 65, 1, 0)

# Save changes to the original file
readmission <- readmission1
rm(readmission1)

```

## Task 2

We will examine the relationship between DRG.Class and DRG.Complication.

```{r}

# Look at DRG.class and DRG.Complication
table(readmission$DRG.Class, readmission$DRG.Complication)
write.csv(table(readmission$DRG.Class,readmission$DRG.Complication), "DRG table.csv")

# Create a backup
readmission2 <- readmission

# Get rid of the appropriate observations
readmission2 <- readmission2[!(readmission2$DRG.Class == "SURG" & readmission2$DRG.Complication == 
                                 "MedicalMCC.CC"),]
table(readmission2$DRG.Class, readmission2$DRG.Complication)

# Combine DRG.Class and DRG.Complication
readmission2$DRG <- paste(readmission2$DRG.Class, "-",readmission2$DRG.Complication)
readmission2$DRG <- as.factor(readmission2$DRG)
levels(readmission2$DRG)

# we will use MED MedicalMCC.CC as the base variable to relevel the variable
table <- as.data.frame(table(readmission2[,"DRG"]))
  max <- which.max(table[,2])
  level.name <- as.character(table[max,1])
  readmission2[,"DRG"] <- relevel(readmission2[,"DRG"], ref = level.name)

table(readmission2[,"DRG"])

# Remove DRG.Class and DRG.Complication
readmission2$DRG.Class <- NULL
readmission2$DRG.Complication <- NULL

# Save changes to original file
readmission <- readmission2
rm(readmission2)

write.csv(table(readmission$DRG), "table-drg.csv")

```


## Task 3

Task 3: Code is provided to perform cluster analysis for from 1 to 12 clusters, construct an elbow plot and create a new variable based on a selected number of clusters. That variable will need to be retained for potentially being added to the dataframe.

```{r}
nstart.val <- 1
cluster_vars <- readmission[c('Log_LOS','Age')]
for(i in 1:ncol(cluster_vars)){
  cluster_vars[,i] <- scale(cluster_vars[,i])
}
km1 <- kmeans(cluster_vars,centers=1,nstart=nstart.val)
km2 <- kmeans(cluster_vars,centers=2,nstart=nstart.val)
km3 <- kmeans(cluster_vars,centers=3,nstart=nstart.val)
km4 <- kmeans(cluster_vars,centers=4,nstart=nstart.val)
km5 <- kmeans(cluster_vars,centers=5,nstart=nstart.val)
km6 <- kmeans(cluster_vars,centers=6,nstart=nstart.val)
km7 <- kmeans(cluster_vars,centers=7,nstart=nstart.val)
km8 <- kmeans(cluster_vars,centers=8,nstart=nstart.val)
km9 <- kmeans(cluster_vars,centers=9,nstart=nstart.val)
km10 <- kmeans(cluster_vars,centers=10,nstart=nstart.val)
km11 <- kmeans(cluster_vars,centers=11,nstart=nstart.val)
km12 <- kmeans(cluster_vars,centers=12,nstart=nstart.val)

var.exp <- data.frame(k = c(1:12),
                      bss_tss = c(km1$betweenss/km1$totss,
                                  km2$betweenss/km2$totss,
                                  km3$betweenss/km3$totss,
                                  km4$betweenss/km4$totss,
                                  km5$betweenss/km5$totss,
                                  km6$betweenss/km6$totss,
                                  km7$betweenss/km7$totss,
                                  km8$betweenss/km8$totss,
                                  km9$betweenss/km9$totss,
                                  km10$betweenss/km10$totss,
                                  km11$betweenss/km11$totss,
                                  km12$betweenss/km12$totss))

ggplot(var.exp,aes(x=k,y=bss_tss))+geom_point() + geom_line()
```

Based on the elbow plot, k = 5 seems to be where the increase in slope slows down dramatically. We will use that to build our cluster

```{r}

LOS_Age_Clust <- as.factor(km5$cluster) #This creates a new variable based on having 8 clusters.
cluster_vars$LOS_Age_Clust <- LOS_Age_Clust
ggplot(data = cluster_vars, aes(x = Age, y = Log_LOS, col = LOS_Age_Clust)) + geom_point() + theme(axis.text = element_blank(), legend.title = element_blank()) +ggtitle("Clustering with 5 groups")

```

## Task 4

Task 4: The following code may help determine if interactions are present. It is best to treat ER as a factor variable for this purpose.

```{r}
#Both variables are factor variables
# Compares Gender and Race to readmission Status
ggplot(readmission,aes(Gender,fill=factor(Readmission.Status))) + geom_bar(position = "fill") +
  facet_wrap(~Race,ncol=2,scales="free")+scale_y_continuous()

#One factor variable and one continuous numeric variable
# Compares ER and log_HCC.Riskscore to Readmission Status
ggplot(readmission,aes(x=factor(Readmission.Status),y=Log_HCC.Riskscore)) + geom_boxplot() +facet_wrap(~factor(ER))

```

## Task 5

I will partition the dataset first into a training / testing dataset

```{r}

#Create train and test sets
library(caret)
set.seed(4321)
partition <- createDataPartition(readmission[,1], list = FALSE, p = .75) #The partition will stratify using variable 1 from the dataframe
train <- readmission[partition, ]
test <- readmission[-partition, ]

print("TRAIN")
mean(train$Readmission.Status)

print("TEST")
mean(test$Readmission.Status)

```

The following code runs a GLM using the logit link and all available variables. Adding an interaction of Gender and Race is included in the code. That is for illustration purposes. The code also produces an ROC curve, a confusion matrix, and calculates AUC.

Model 1: fitting a logit link function

```{r}
library(pROC)
glmlogit <- glm(Readmission.Status ~ . + Gender*Race, data=train, family = binomial(link="logit"))

summary(glmlogit)

predslogit <- predict(glmlogit,newdat=test,type="response")

roclogit <- roc(test$Readmission.Status,predslogit)

confusionMatrix(factor(1*(predslogit>.5)),factor(test$Readmission.Status))
#Accuracy = 0.8769

plot(roclogit)

auc(roclogit)
#auc = 0.7324

```

Model 2: fitting a probit link function
```{r}

glmprobit <- glm(Readmission.Status ~ . + Gender*Race, data=train, family = binomial(link="probit"))

summary(glmprobit)

predsprobit <- predict(glmprobit,newdat=test,type="response")

rocprobit <- roc(test$Readmission.Status,predsprobit)

confusionMatrix(factor(1*(predsprobit>.5)),factor(test$Readmission.Status))
# Accuracy = 0.877

plot(rocprobit)

auc(rocprobit)
#auc = 0.7324

aucvalues <- data.frame("Link Function" = c("Logit","Probit"),
                        "AUC Values" = c(auc(roclogit),auc(rocprobit)),
                        "Accuracy" = c(0.8769,0.877))

write.csv(aucvalues,"aucvalues.csv")

```

## Task 6

we will create a new dataset using the cluster variable

```{r}

# create a backup of the dataset
readmission.clust <- readmission

# Put the new variable into the dataset
readmission.clust$LOS_Age_Clust <- LOS_Age_Clust

# remove the Age and Log_LOS variables
readmission.clust$Log_LOS <- NULL
readmission.clust$Age <- NULL

# partition the new dataset
train <- readmission.clust[partition,]
test <- readmission.clust[-partition,]



```


```{r}

glmclust <- glm(Readmission.Status ~ . + Gender*Race, data=train, family = binomial(link="logit"))

summary(glmclust)

predsclust <- predict(glmclust,newdat=test,type="response")

rocclust <- roc(test$Readmission.Status,predsclust)

confusionMatrix(factor(1*(predsclust>.5)),factor(test$Readmission.Status))
#Accuracy = 0.8771

plot(rocclust)

auc(rocclust)
#auc = 0.7324

clust_comparison <- data.frame("Link Function" = c("Original GLM","Clustered GLM"),
                               "AUC Values" = c(auc(roclogit),auc(rocclust)),
                               "Accuracy" = c(0.8769,0.8771))

write.csv(clust_comparison,"clust_comparison.csv")

```

## Task 7

We will use StepAIC to further improve the predictive accuracy of the model and remove variables with low predictive power. Before we do this we will binarize certain categorical variables for better analysis.

```{r}

# binarizez the following variables: Gender, Race, DRG
library(caret)

# convert the variables to character as caret cannot handle non-character variables
factor_names <- c("Gender","Race","DRG") #insert the column names of the variables to be binarized
factor_vars <- readmission[,factor_names]
for (var in factor_names) {
  factor_vars[, var] <- as.character(factor_vars[, var])
}

# binarize the variables and predict the values
binarizer <- caret::dummyVars(paste("~", paste(factor_names, collapse = "+")) , data = factor_vars, fullRank = TRUE)
binarized_vars <- data.frame(predict(binarizer, newdata = factor_vars))
head(binarized_vars)

```

Create a new dataset which includes the binarized variables. we will remove the original variables and the base variables

```{r}

# Create a backup dataset
readmission.bin <- readmission

# Include Binarized variables into the new dataset
readmission.bin <- cbind(readmission.bin, binarized_vars)

# remove the original variables
readmission.bin$Gender <- NULL
readmission.bin$Race <- NULL
readmission.bin$DRG <- NULL

# partition the dataset
train <- readmission.bin[partition,]
test <- readmission.bin[-partition,]

```

Create a new glm using the binarized dataset

```{r}

glmbin <- glm(Readmission.Status ~ ., data=train, family = binomial(link="logit"))

summary(glmbin)

predsbin <- predict(glmbin,newdat=test,type="response")

rocbin <- roc(test$Readmission.Status,predsbin)

confusionMatrix(factor(1*(predsbin>.5)),factor(test$Readmission.Status))
#Accuracy = 0.877

plot(rocbin)

auc(rocbin)
#auc = 0.733

```

Now we will use stepAIC to remove certain variables
```{r}

library(MASS)
stepAIC(glmbin)

```

We will now create a new model using the formula from stepAIC

```{r}

form1 <- as.formula(Readmission.Status ~ Age + Log_LOS + Log_HCC.Riskscore + DRGMED...Other +
                      DRGSURG...Other)

glmbin <- glm(form1, data=train, family = binomial(link="logit"))

summary(glmbin)

predsbin <- predict(glmbin,newdat=test,type="response")

rocbin <- roc(test$Readmission.Status,predsbin)

confusionMatrix(factor(1*(predsbin>.5)),factor(test$Readmission.Status))
#Accuracy = 0.877

plot(rocbin)

auc(rocbin)
#auc = 0.7335

bin_comparison <- data.frame("Link Function" = c("Original GLM","Clustered GLM", "Binarized GLM"),
                               "AUC Values" = c(auc(roclogit),auc(rocclust), auc(rocbin)),
                               "Accuracy" = c(0.8769,0.8771, 0.877))

write.csv(bin_comparison,"bin_comparison.csv")

```

## Task 8

I will run the model on the full dataset

```{r}

form1 <- as.formula(Readmission.Status ~ Age + Log_LOS + Log_HCC.Riskscore + DRGMED...Other +
                      DRGSURG...Other)

glmfinal <- glm(form1, data=readmission.bin, family = binomial(link="logit"))

summary(glmfinal)
str(readmission.bin)

```

Create randomized data to be used for results and assess the final model

```{r}
new.data <- data.frame("Log_LOS" = c(log(5),log(6),log(5),log(5),log(5),log(5)), "Age" = c(75,75,80,75,75,75), "Log_HCC.Riskscore" = c(log(1.866),log(1.866),log(1.866),log(2.053),log(1.866),log(1.866)), "DRGSURG...Other" = c(0,0,0,0,1,0), "DRGMED...Other" = c(0,0,0,0,0,1))
new.data
x <- predict(glmfinal, newdat = new.data, type = "response")

final.data <- cbind(new.data, "probability" = x)
write.csv(final.data,"finaldata.csv")

```


## Task 8

Task 9: The following code calculates the cost using an initial cutoff of 0.075. In order to better assess the affect of the cutoff on the model, we have used various cutoff values.

It assumes the final model constructed on the full dataset is called glm_full and the final dataset is readmit.

```{r}

# Name the dataset and model appropriately
glm_full <- glmfinal
readmit <- readmission.bin

cutoff_values <- c(0,0.05,0.07,0.075,0.08,0.09,0.1,0.2,0.3,0.4,1)

pred_full <- predict(glm_full,type="response")
cutoff <- .075
pred_readmit <- 1*(pred_full > cutoff)
cm <- confusionMatrix(factor(pred_readmit),factor(readmit$Readmission.Status))

no_intervention_cost <- 25*sum(readmit$Readmission.Status == 1)
full_intervention_cost <- 4*sum(pred_readmit)
modified_cost <- cm$table[2,1]*4+cm$table[2,2]*4+cm$table[1,2]*25
no_intervention_cost
full_intervention_cost
modified_cost


```

